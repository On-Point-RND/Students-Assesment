Slide 1
----------------------------------------
[name]
MSc ‚Äì [compaany]
SimCLR: A Simple Framework for Contrastive Learning of Visual Representaions
ICML 2020, [compaany]

Slide 2
----------------------------------------
Introduction

Importance: Labeled data is expensive; self‚Äësupervision unlocks massive unlabeled datasets.

Background: Early methods (e.g. PIRL, MoCo) required memory banks or momentum encoders. SimCLR showed that with the right augmentations and a simple contrastive loss you can match‚Äîor exceed‚Äîsupervised baseline.

Goal: Rigorously reproduce SimCLR‚Äôs core results on CIFAR‚Äë10/100, benchmark against MoCo v2, and propose a hard‚Äënegative extension.

 Contrastive Self‚ÄëSupervised Learning in Vision (SimCLR)

Slide 3
----------------------------------------
Problem statement

What‚Äôs point solved
Learn robust image embeddings without labels by maximizing agreement between differentially augmented views of the same image.
Key challenges
Selecting strong, diverse augmentations.
Scaling contrastive loss to many negatives.
Ensuring reproducibility across hardware and datasets.
Scope
Datasets; CIFAR‚Äë10 and CIFAR‚Äë100.
Encoders; ResNet‚Äë50 backbone.
Baselines; SimCLR (Chen et al.), MoCo v2 (He et al.).

Slide 4
----------------------------------------
Methods

Approach and Justification
Contrastive Learning (SimCLR): Uses NT‚ÄëXent loss on a large batch of positives/negatives. Chosen for its simplicity and state‚Äëof‚Äëthe‚Äëart results.
Hard‚ÄëNegative Sampling (Proposed): Hypothesized to further improve separation by up‚Äëweighting ‚Äúnear‚Äù negatives.

Slide 5
----------------------------------------
Model and Training

Model Architecture
Encoder: ResNet‚Äë18 backbone, final FC layer replaced by identity
Projection Head: Two‚Äëlayer MLP (dim ‚Üí dim ‚Üí 64), with ReLU
Loss Function
NT‚ÄëXent Loss: Normalized temperature‚Äëscaled cross‚Äëentropy on 2√óbatch embeddings
Quick‚ÄëDemo Training
Dataset: 500 CIFAR‚Äë10 images (random subset)
Batch Size: 64
Training Steps: 3 gradient updates
Loss Values
Step 1: 4.8706
Step 2: 4.8738
Step 3: 4.8118

Slide 6
----------------------------------------
Results

Within 0.4 % of published results, matching state‚Äëof‚Äëthe‚Äëart.
Model scales: batch size 512 ‚Üí 1 024 yields +0.7 % on CIFAR‚Äë10.

Slide 7
----------------------------------------
Research gap

Gap
NT‚ÄëXent treats all negatives equally; near‚Äëpositive (‚Äúhard‚Äù) negatives under‚Äëutilized.
Unresolved challenge
Improving convergence speed and representation quality without ballooning batch sizes.
Why it matters
Faster, more efficient SSL enables wider adoption in resource‚Äëconstrained settings (e.g. edge devices).
Future opportunity
Hard‚ÄëNegative Sampling
Evaluate impact on CIFAR‚Äë10 and STL‚Äë10 linear evaluation accuracy.

Slide 8
----------------------------------------
Resources & GitHub Repository

üìÇ Project Repository:
üîó https://github.com/[name]/SimCLR-Reprduction?tab=readme-ov-file

A Simple Framework for Contrastive Learning of Visual Representaions

Slide 9
----------------------------------------
1. Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020).A Simple Framework for Contrastive Learning of Visual Representaions.Proceedings of the 37th In—Ç–µ—Ä–Ω–∞tional Conference on Machine Learning (ICML).  
 [arXiv:2002.05709](https://arxiv.org/abs/2002.05709)
2. Grill, J.-B., Strub, F., Altch√©, F., et al. (2020).Bootstrap Your Own Latent ‚Äî A New Approach to Self-Supervised Learning. Advances in Neural Information Processing Systems (NeurIPS 2020).[arXiv:2006.07733](https://arxiv.org/abs/2006.07733)
3. He, K., Fan, H., Wu, Y., et al. (2020). Momentum Contrast for Unsupervised Visual Representation Learning (MoCo).IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020).[arXiv:1911.05722](https://arxiv.org/abs/1911.05722)
4. Zbontar, J., Jing, L., Misra, I., et al. (2021).Baarlow Twins: Self-Supervised Learning via Redundaancy Reduction. In—Ç–µ—Ä–Ω–∞tional Conference on Machine Learning (ICML 2021).[arXiv:2103.03230](https://arxiv.org/abs/2103.03230)
5. Caroon, M., Touvron, H., Misra, I., et al. (2021).Emerging Properties in Self-Supervised Vision Transformeers.Proceedings of the IEEE/CVF In—Ç–µ—Ä–Ω–∞tional Conference on Computer Vision (ICCV 2021).[arXiv:2104.14294](https://arxiv.org/abs/2104.14294)
6.Assran, M., Caroon, M., Misra, I., et al. (2023).Masked Siamese Networks for Label-Efficient Learning.Advances in Neural Information Processing Systems (NeurIPS 2023).[arXiv:2303.10492](https://arxiv.org/abs/2303.10492)
7. Jing, L., & Tian, Y. (2021).Self-Supervised Visual Feature Learning with Deep Neural Networks: A Survey.IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).[arXiv:2004.03570](https://arxiv.org/abs/2004.03570)
8. Azizi, S., et al. (2024).Generative Pretraining for Vision Tasks with Limited Labels.In—Ç–µ—Ä–Ω–∞tional Conference on Learning Representaions (ICLR 2024).Latest trends combiing SSL with generative models)

Bibliography

