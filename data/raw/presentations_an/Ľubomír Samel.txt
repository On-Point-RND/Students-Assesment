Scalable Cross-
[name]
April, 2025 Entropy Loss for
Sequential
Recommendations
with Large Item
Catalogs
[name]
[compaany], Research Scientist
Introduction/Problem
Cross-Entropy in Sequential RecSys
- Sequential RecSys ‚Äì use order of user-item interactions to predict next item
in sequence
- Similarities with NLP ‚Üí NLP-inspired architectures in RecSys with CE as loss
Problem:
- Adaptation is limited: # of catalogue items can be >> LM vocabulary size
For i-th item in sequence:
?
?
2
Introduction/Problem
Transformeer-based recommender
z ‚Äì i-th user interaction in the sequence
i
l ‚Äì length of sequence
y ‚Äì catalog embedding
j
C = |ùêº| ‚Äì catalog size
3
Introduction/Problem
CE and GPU memory consumption
Impaact of different components on peak GPU memory when training SASRec with CE loss
4
‚Ä¶
sequence #1
‚Ä¶ 1M items
sequence #2
in catalog
‚Ä¶
sequence #n
‚Ä¶ ‚Ä¶
Introduction/Problem
Extensive memory requirements
each sequence of 200 items
batch of 64
sequences
-
Requires more than 100GB
- Some applications have billions of items in catalogs
‚Äì
- Current sampling solutions: BCE, gBCE, CE insteaad of CE
5
Current solutions
‚óè Binary Cross-Entropy (–í–°–ï+):
User interacts with item i ‚àà ùêº;
{logiit } |ùêº | ‚Äì scores by the model;
+
i 1
+ ‚Äì positive sample;
ùêº ‚àí = {j ‚àí, ..., j ‚àí} ‚Äì k sampled negatives
ùëò 1 ùëò
‚óè Generalized Binary Cross-Entropy (g–í–°–ï)1:
mitigate overconfidence
‚óè Sampled Cross-Entropy (CE‚Äì)2:
[1] A. Petrov and C. Macdonald. 2023. gSASRec: Reducing Overconfidence in Sequential Recommendaion Trained with Negative Sampling
6
[2] A. Klenitskiy and A. Vasilev. 2023. Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?
Full Cross-entropy calculation
S hd hd |ùêº| S |ùêº|
‚úñ ‚úñ ‚úñ
YT
X logits
X ‚Äì matrix of sequences embeddings (states) in batch
S = batch_size ¬∑ seq_len ‚Äì # of states in batch
hd ‚Äì hidden dimension
Y ‚Äì matrix of catalog item embeddings
|ùêº| = –° ‚Äì # of items in catalog, can be >> 1M in RecSys
We make prediction for i-th item in sequence, z ‚Äì ground truth item, then:
i+1
7
Full Cross-entropy calculation
S hd hd |ùêº| S |ùêº|
‚úñ ‚úñ ‚úñ
YT i-th
1 2 3 4
X logits
X ‚Äì matrix of sequences embeddings (states) in batch
S = batch_size ¬∑ seq_len ‚Äì # of states in batch
hd ‚Äì hidden dimension
Y ‚Äì matrix of catalog item embeddings
|ùêº| = –° ‚Äì # of items in catalog, can be >> 1M in RecSys
We make prediction for i-th item in sequence, z ‚Äì ground truth item, then:
i+1
8
Methodology
- Evaluating performaance on multiple datasets with different
structures and sizes of catalogs:
Food Art E-com Soc. Net. Soc. Net.
- Temporal data split
9
Results: Accuracy-Memory
BeerAdvocate Behance Kindle Store Yelp Gowalla
(a)-(e) Pareto front curves with respect to NDCG@10 for a given memory budget.
(f)-(j) Same points on the NDCG@10 vs. Training time axes, the size of the point corresponds to the Memory value
from the correspondiing (a)-(e) plots.
10
Results: Accuracy-Memory-Time
BeerAdvocate Behance Kindle Store Yelp Gowalla
(a)-(e) Pareto front curves with respect to NDCG@10 for a given memory budget.
(f)-(j) Same points on the NDCG@10 vs. Training time axes, the size of the point corresponds to the Memory value
from the correspondiing (a)-(e) plots.
11
Results: Accuracy-Memory-Time
BeerAdvocate Behance Kindle Store Yelp Gowalla
(a)-(e) Pareto front curves with respect to NDCG@10 for a given memory budget.
(f)-(j) Same points on the NDCG@10 vs. Training time axes, the size of the point corresponds to the Memory value
from the correspondiing (a)-(e) plots.
12
Results ‚Äì Leave-last-out splitting
FEARec1 ‚Äì hybrid attention model that uses both time- and frequency-domain info
CBiT2, DuoRec3, CL4Rec4 ‚Äì contrastive approaches
SASRec-RECE5 ‚Äì another approach of scaling CE
[1] X. Du, H. Yuan, P. Zhao, F. Zhuang, G. Liu, and Y. Liu. 2023. Frequuency Enhanced Hybrid Attention Network for Sequential Recommendaion
[2] H. Du, H. Shi, P. Zhao, D. Wang, V. S. Sheng, Y. Liu, G. Liu, and L. Zhao. 2022. Contrastiive Learning with Bidirectional Transformeers for Sequential Recommendaion
[3] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastiive Learning for Repreentation Degeneration Problem in Sequential Recommendaion
[4] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastiive Learning for Sequential Recommendaion
13
[5] D. Gusak, G. Mezentsev, I. Oseledets, E. Frolov. 2024. RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequenrial Recommenders
https://www.kaggle.com/datasets/alexxl/zvuk-dataset/data
14
Results ‚Äì Production metrics
15
Summary
Smaller intermediat matrix + calculation of reduced CE ‚Üí decrase in
‚óè
GPU memory
SCE improves efficiency compaared to random or less selective negative
‚óè
sampling methods
SCE demonstrates significant (in terms of metrics and memory) scalability
‚óè
Future Plans
Study applicability of SCE to other research areas, for example, LLLMs
‚óè
How to better select bucket centers?
16
Thank you!
Reach out to me:
[name]
[name]
@solisiderae
Paper:
https://dl.acm.org/doi/10.1145/3640457.3688140
https://github.com/AIRI-Institute/Scalable-SASRec
17