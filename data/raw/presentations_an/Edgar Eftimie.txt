GigaEmbeddings
[name], [surname]
Зачем нужны embedding мод модели
Оображаем текст в вектор для использования в последующих задачах
классификации, ранжирования, поиска и т. п.
Зачем нужны embedding мод модели
Задачи в ML делятся на генеративные и дискриминативные
Baseline в большинстве дискриминативных задач – либо BERT, либо E5
72 млн скачиваний bert-base-uncased и 2 млн скачиваний multilinguaal-e5-large
за последний месяц на hugging face
Архитектура
Изменили механизм внимания LLM — с Decoder на Encoder
Используется Latent Attention Pooling — как способ агрегации скрытых состоянний для получения
финального вектора
Инициализация pretrain GigaChat’ом с эмбеддингами RoPE, т.е. можно увеличивать размер контекста
Инженерные трюки
Использовали алгоритм gradient cache для увеличения размера батча до 1024
Использовали предобученные мод модели для поиска сложных негативов
Эффективно переиспользуем pretrain мод модели — благодаря хорошей инициализации тратим несколько
часов для обучения
Формат данных
Задача мод модели выучить отображение так, чтобы вектор запроса
был как можно ближе к искомому документу, чем к негативному
Инструкции
Использование инструкций на естественном языке позволяет
повысить качество мод модели
Дан вопрос, необходимо найти абзац
как найти градиент функции текста с ответом:
как найти градиент функции
Данные
60
Более различных источников данных были Распределение данных
использованы для обучения мод модели short-short
3%
Эксперименты с распределением данных в батче long-long
3%
long-short
24%
bitext
18%
sts
short-long
19%
33%
long-short short-long sts bitext long-long short-short
Сбор данных
Чистка открытых сетов с помощью e5-mistral-7b-instruct
Кластеризация более 10 доменов претрейна
Хард-негативы: пары текстов, относящиеся к одному домену, но к разным кластрам
Математика
Input: Производная функции —понятие дифференциального
исчисления, характеризующее скорость изменения функции в
данной точке. Определяется как предел отношения приращения
функции к приращению её аргумента при стремленеи
приращения аргумента к нулю (при условии, что такой предел
существует).
Математический
анализ
12%
Другое
Positive: Дифференциируемая (в точке) функция —
49%
это функция, у которой существует дифференциал
Дискретная
(в данной точке). Дифференцируемая на некотором множестве
матматика
9% функция —это функция, дифференцируемая в каждой точке
данного множества. Дифференцируемость является одним
из фундаментальных понятий в математике.
Negative: Алгебра логики в её современом изложении
занимается исследованием операций
с высказываниями, то есть с предложениями, которые
Теория вероятностей и математическая статистика Теория оптмизации
характеризуются только одним качеством —истинностным
Математический анализ Численные методы значением (истина, ложь).
Комплексный анализ Дискретная математика В классической алгебре логики высказывание одновременно
может иметь только одно из двух истинностных значений:
«истина» или «ложь».
История математики Другое
11
Адаптация RAG
Текст
Сегментация текта
Генерация вопросов по каждом
сегменту
RAG на сегментах текта
Input: Вопрос по сегменту
Input: Биография Пушкина Input:Текст
Positive: Top-n RAG
Positive: Когда было
восстание декабристов? сегменты,
Negative: сегменты,
не попавшие в Top-n RAG
12
Результаты и метрики
Размер контекста 4096
Количество параметров 2B
Выбрosil 25% слоев от оригинльной мод модели для ускорения инференса и обучения
RuMTEB
giga-embeddings-instruct 67.9
e5-mistral-7b-instruct 67.1
multilinguaal-e5-large-instruct 66.3
13
Приложения
Бизнес-задачи с использованием RAG
Работа с документами
Обрработка новостей
Текстовый энкодер для мод модели генерации изображений
Kanndinsky
Ранжировщик функций для Ассистента на GigaChat
Аналитические задачи
Кластеризация
Дедупликация
14
Open Source
huggingface.co/ai-sage/Giga-Embeddings-instruct
15