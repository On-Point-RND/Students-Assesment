ReAct: Synergizing
Reasoning and
Acting in
Language Models
by [name] [surname] et al. (2023)
Articleâ€™s overview,
results reproduction and new findings
by [name],
3rd year student of
[location] [university],
Faculty of Mathematics and Mechanics
The ReAct Paradigm
General setup:
Observation ğ‘œ âˆˆ O => action ğ‘ âˆˆ A following some policy Ï€(ğ‘ |ğ‘ ), where ğ‘ = (ğ‘œ , ğ‘ , Â· Â· Â· ,
ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ 1 1
ğ‘œ , ğ‘ , ğ‘œ ) is the context to the agent.
ğ‘¡âˆ’1 ğ‘¡âˆ’1 ğ‘¡
ReActâ€™s setup:
Augmented agentâ€™s action space to A* = A âˆª L, where L is the space of language.
action ğ‘ âˆˆ L in the language space (thought) => no observation feedback.
ğ‘¡
ğ‘ => reasoning over the current context ğ‘ => update the context ğ‘ = (ğ‘ , ğ‘ )
ğ‘¡ ğ‘¡ ğ‘¡+1 ğ‘¡ ğ‘¡
Methodology
â€¢ Prompt Design
Few-shot examples demonstrate ReAct's
thought-action format.
â€¢ Action Space
Task-specific actions (e.g., search, answer) are
predefined.
â€¢ Environment
External systems execute actions and return
observations.
â€¢ Execution
Model alternates thoughts and actions until task
completion.
Benchmark Tasks
â€¢ Question Answering (HotpotQA): A â€¢ Text-based Game (ALFWorld): Navigating and
multi-hop QA dataset requiring completing household tasks in a text-based
reasoning across multiple pieces of
virtual environment (organized text environment)
information
â€¢ Web Shopping (WebShop): Finding products
â€¢ Fact Verification (FEVER):
Determining whether claims are on a simulated e-commerce website based on supported, refuted, or unverified natural language instructions (noisy
based on evidence environment, unstructured texts)
The authors used frozen PaLM-540B (prompted with few-shot in-context examples)
Key Results: HotpotQA, FEVER
CoT (CoT-SC) vs ReAct
* CoT-SC (self-consistent CoT) - sampling 21 CoT trajectories
with decoding temperature 0.7 during inference and adopting
the majority answer
Key Results: HotpotQA, FEVER
*Finetuning PaLM-8b, -62b
Reproduction of experiments
Setup:
â€¢ Qwen-8b (quantized,
temperature 0.1)
â€¢ ReAct framework (initial authorâ€™s
version)
â€¢ CoT (Qwen-8b prompted to think
step-by-step)
â€¢ 200 questions/claims
â€¢ Learning = prompt
HotpotQA Benchmark
â€¢ CoT outperforms
HotpotQA
â€¢ Label ambiguity
HotpotQA Benchmark
â€¢ Correlation between number of steps
and EM / answerâ€™s length
HotpotQA Benchmark
â€¢ Optimal number of steps < 5
â€¢ Entering infinite loops (reaching
maximum 8 steps) leads to
empty string as an answer
HotpotQA Benchmark
â€¢ Three-step
trajectory results in
longer answer (?)
FEVER Benchmark
â€¢ Without proper prompting, agent becomes trapped in iterative loops of unproducive
reasoning or action sequences.
â€¢ Solution: add to context a message to follow strict pattern
(e. g. â€œIn the last line of your answer write only in format: Action i: Finish[], nothing else.â€)
Conclusions
In summary:
â€¢ Reproduced results align with the
original authors' findings.
â€¢ ReAct demonstrates superior
performance over Chain-of-
Thought (CoT) when given
optimal prompting or fine-tuning.
â€¢ Agent performance scales with its
ability to learn the correct
environment interaction format.
Thank you!
All results and code are available on
Github: https://github.com/[name][surname]/ReAct_experiments