A review of multimodal methods
Introduction
Modality refers to the way something happens or is experienced, and a research problem is described as multimodal when it contains multiple such modalities. Modality refers to the way things happen or are experienced, and a research problem is described as multimodal when it contains multiple modalities. Multimodal AI utilizes data from multiple different modalities (e.g., text, images, sound, video, etc.) to learn and reason. Multimodal AI emphasizes the complementarity and fusion between data from different modalities, and promotes the comprehensive development of intelligent applications by integrating data from multiple modalities and utilizing techniques such as representation learning, modal fusion and alignment to achieve cross-modal perception, understanding and generation.
Problem formulation
Multimodal fusion is a technique for combining different types of data (e.g., images, text, audio and video, etc.) to improve the understanding and generation of models, etc. However, this process is full of difficulties and challenges, and the following are common difficulties in multimodal fusion:
1. Inconsistency between modalities. Data in different modalities have different feature dimensions and representations; for example, images are high latitude pixel values, while text is discrete word vectors. Also data from different modalities may differ in the number and scales may differ, which may lead to imbalance problems during fusion.
2. Complexity of information fusion. Information redundancy and loss, data from different modalities may contain redundant information or lose critical information, and need to be fused effectively to avoid information loss . Selection of fusion strategy, Choosing the appropriate fusion strategy (e.g., early fusion, late fusion, mid-term fusion) has a significant impact on the results, and different tasks may require different strategies .
3. Data Alignment and Coherence. Data from different modalities may not be temporally aligned, e.g., video and speech data inconsistent data formats, image data may be two-dimensional while text data is linear .
4. Computational resources and efficiency. Multimodal fusion usually requires processing high-dimensional data and complex computations, leading to high computational demands.
5. Semantic differences between modalities. Each modality may represent different semantic information, and it may be difficult to capture the complex semantic relationships between modalities during fusion.
6. Model Interpretability. The complexity of a multimodal fusion model may make the model's decision-making process difficult to understand, and understanding and interpreting the output of a multimodal model requires additional work .
Methods
Radford et al.'s paper published in ICML 2021 proposes the CLIP model (Contrastive Language-Image Pretraining), which pioneers a new paradigm for learning relatable visual models through natural language supervision[1]. Traditional computer vision models rely on fixed category labels for supervised learning, limiting the generalization ability and flexibility of the models. This article utilizes natural language as a supervised signal instead of fixed category labels, learns to align the representation space of images and text by comparison, and constructs a large-scale image-text pair dataset for pre-training.
The architecture of CLIP, shown in Fig. 1. The CLIP framework employs a dual-encoder contrastive learning architecture designed to align visual and linguistic representations through large-scale pretraining on image-text pairs. The system consists of two parallel encoders: an image encoder, implemented either as a ResNet-based CNN or a Vision Transformer (ViT), and a text encoder, built upon a standard Transformer architecture.
Given a batch of ùëÅ image-text pairs, the image encoder produces features ùêº ‚àà ‚ÑùùëÅ√óùëë , and the text encoder produces features ùëá ‚àà ‚ÑùùëÅ√óùëë . The similarity matrix SS is computed as the dot product between all image and text features:
ùëÜ = ùêº ‚ãÖ ùëá
ùëñ,ùëó ùëñ ùëó
Figure 1 - The architecture of CLIP.
The training objective employs a symmetric contrastive loss, where a temperature-scaled softmax operation (controlled by a learnable parameter œÑ) distinguishes positive pairs from negatives. For the image-to-text contrastive loss, each image feature is compared against all text features in the batch, and a cross-entropy loss maximizes its similarity with the correct paired text. An analogous text-to-image contrastive loss is computed in the reverse direction, and the final loss is the average of the two. This symmetric alignment mechanism forces the model to project both modalities into a shared embedding space where semantically related images and texts are close.
Existing visual-linguistic models (e.g., CLIP based on contrast learning), while performing well in zero-shot scenarios, can only output similarity scores between images and text, and are unable to generate language. The goal of Flamingo is to fill this gap by constructing a model that is capable of handling multimodal inputs and generating linguistic outputs at the same time. Flamingo is a visually conditioned autoregressive text generation model capable of accepting arbitrarily interleaved image/video and text sequences as input and generating text as output[2].
The architecture of Flamingo, shown in Fig. 2, combines pre-trained visual and linguistic models and bridges the two through innovative components:
Vision Encoder: outputs spatio-temporal features of an image or video using a pre-trained Normalizer-Free ResNet (NFNet), pre-trained by contrast learning.
Perceiver Resampler: transforms the features of the Vision Encoder (which may be high-dimensional and variable in number) into a fixed number of visual tokens (typically 64), reducing computational complexity and adapting to the language model.
Frozen LM: Based on Chinchilla, a 70B-parameter language model, the incorporation of the decoding task trigger and employ multimodal causal self-attention masks to control information flow. This design ensures the query vectors capture all visual information needed for text generation and gradually pass it to text tokens via self-attention.
The image-text contrastive learning (ITC) task aims to align cross-modal representation spaces. In this task, the model calculates the similarity between the query representations Z output by the image Transformer and the [CLS] token representations output by the text Transformer. By comparing the similarities of positive and negative sample pairs, feature alignment is achieved. The image-conditioned text generation (I TG) task focuses on training the model to generate relevant text based on visual input. For this task, the authors replace the [CLS] token with a [DEC] token as the decoding task trigger and employ multimodal causal self-attention masks to control information flow. This design ensures the query vectors capture all visual information needed for text generation and gradually pass it to text tokens via self-attention. The image-text matching (ITM) task aims to learn fine-grained cross-modal alignment. Here, the authors use bidirectional self-attention masks to make all queries and text mutually visible, allowing the output query embeddings Z to fully integrate multimodal information.
After pre-training the Q-Former, the authors connect it to a large language model (LLM). The query embeddings Z are mapped to the LLM's word embedding space via a fully connected layer and input as soft visual prompts. Since the Q-Former has already learned to extract text-relevant visual representations during pre-training, this significantly reduces the burden on the LLM to learn visual-text alignment. While Flamingo demonstrates efficient advantages in vision-language pretraining by leveraging frozen image encoders and LLMs to achieve state-of-the-art performance across multiple tasks. Its performance remains constrained in complex reasoning tasks requiring deep integration of visual concepts with language. Furthermore, the current architecture is specifically designed for static images, making it difficult to process temporal visual data such as videos.
[1] Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision[C]//International conference on machine learning. PmLR, 2021: 8748-8763.
[2] Alayrac J B, Donahue J, Luc P, et al. Flamingo: a visual language model for few-shot learning[J]. Advances in neural information processing systems, 2022, 35: 23716-23736.
[3] Li J, Li D, Savarese S, et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models[C]//International conference on machine learning. PMLR, 2023: 19730-19742.