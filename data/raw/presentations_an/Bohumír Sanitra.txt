

Protecting Privacy in 3D Point Clouds:  
Reproducing "Unlearnable 3D Point Clouds"  
via Class-wise Transformations  

The Problem of Unauthorized Use of 3D Data  
Context:  
● 3D point clouds are widely used in autonomous driving, robotics, and AR/VR.  
● These datasets often contain sensitive personal or environmental information (e.g., people, vehicles, private spaces).  
Threat:  
● If leaked or intercepted, 3D data can be used to train unauthorized deep learning models.  
● Unauthorized usage leads to privacy violations, intellectual property loss, and security breaches.  
Why 3D Needs Protection:  
● Unlike 2D images, 3D point clouds are structurally complex and spatially rich.  
● Standard image protection techniques cannot be directly applied.  

Existing Data Protection Techniques and Their Limitations  
For 2D Data:  
● Unlearnable examples have been introduced: perturbations prevent DNNs from learning useful representations.  
● Techniques include model-dependent and model-agnostic methods.  
Challenges with 3D Point Clouds:  
● Incompatibility: 2D perturbations rely on pixel space; point clouds are coordinate-based.  
● Visual degradation: Heavy perturbation can distort 3D object geometry.  
● No restoration: Many methods block all users, including authorized ones, from learning.  
Conclusion:  
A new, 3D-native, and reversible privacy protection scheme is urgently needed.  

Existing Methods for Generating Unlearnable Examples in  
2D  
Key Techniques in 2D:  
● Error-maximizing noise: Makes models learn misleading patterns.  
● Feature collision: Forces protected samples to share features with other classes.  
● Model-specific masking: Exploits weaknesses in target architecture.  
Limitations:  
● Require fine-tuned control over image pixels.  
● Often lack generalization to new models.  
● Mostly irreversible—authorized users are blocked too.  
Bottom Line:  
2D methods are not directly transferable to 3D point clouds due to modality differences and transformation complexity.  

Limitations of Existing Methods When Applied to 3D Data  
● Most 2D unlearnable methods rely on pixel-level manipulation, which is not  
applicable to unordered point cloud data.  
● Applying heavy perturbations to 3D point clouds may destroy geometric and  
semantic structures.  
● Visual quality degradation is more severe in 3D due to spatial distortions.  
● 2D methods often target specific models and lack generalization to diverse 3D  
architectures.  
● They do not address the restoration problem: even authorized users cannot learn  
from protected data.  

Class-wise Transformations for Creating Unlearnable 3D  
Point Clouds  
Data Restoration Mechanism for Authorized Users  
● The method supports reversibility via invertible transformation matrices.  
● Authorized users receive lightweight metadata (e.g., transformation  
parameters).  
● The original point cloud data is recovered by applying the inverse of the  
transformation.  
● This enables authorized training on clean-equivalent data while preserving  
protection from unauthorized use.  
● The approach bridges the gap between data privacy and usability.  

Environment Setup and Code Execution Process  
The official codebase was obtained from the authors’ GitHub:  
https://github.com/[company]/UnlearnablePC  
Environment setup:  
● Python 3.8, CUDA 11.3, PyTorch 1.10+  
● Dependencies installed via pip and conda  
● Optional: visualization tools (e.g., open3d, matplotlib)  
Execution steps:  
● Clone the repository and set up the virtual environment  
● Download datasets (ModelNet10/40, ShapeNetPart, etc.)  
● Run training with clean, transformed, and restored data using provided scripts:  
python train.py --dataset modelnet40 --mode UMT --trans RS  

Experimental Results on Various Datasets and Models  
Evaluated on 6 datasets:  
● Synthetic: ModelNet10, ModelNet40, ShapeNetPart  
● Real-world: KITTI, ScanObjectNN, S3DIS  
Tested with 16 3D models:  
● PointNet, DGCNN, PointCNN, PointMLP, RIConv++, etc.  
Key results:  
● UMT (k=2: rotation + scaling) significantly reduces model accuracy:  
○ From ~90% to ~20% on classification tasks  
○ From ~76% to ~36% eval accuracy on segmentation tasks  
Restoration success:  
● Authorized users applying inverse transformations restore original performance nearly to baseline.  

Analysis of Method Effectiveness  
● UMT significantly decreases unauthorized learning accuracy across multiple  
datasets and models.  
● Class-wise setting is more effective than sample-wise or dataset-wise strategies.  
● The proposed method is robust to standard data augmentations (jitter, scaling,  
rotation).  
● Restoration for authorized users leads to accuracy close to clean baseline.  
● Theoretical analysis confirms UMT’s effect under GMM and Bayes optimal decision  
boundary assumptions.  

Possible Improvements and Future Research Directions  
Current limitations:  
● Vulnerable to models with strong transformation invariance (e.g., RIConv++)  
● Focused only on classification and semantic segmentation  
Possible extensions:  
● Explore protection against point cloud completion and reconstruction tasks  
● Investigate robustness under adversarial settings or with unknown test-time augmentations  
Ideas for improvement:  
● Add new non-rigid transformations (e.g., bending, occlusion simulation)  
● Use learnable transformation generation via adversarial networks  
● Combine with cryptographic data access policies  
Open-source contribution:  
● Future plan: refactor code into a plug-and-play privacy wrapper for point cloud datasets  

Links to research reproduction repository and sources  
1. Research reproduction repository  
2. Original research repository