Topological Analysis of Mathematical Proofs Generated by Large
Language Models in the Lean 4 Theorem Proving Language
[name]
Data Scientiist, [compaany] (Department of
Artificial Intelligence and Machine Learning
Development)
academic advisor: [surname]
Introduction
Large language models (LLMs) are increasingly applied to tasks that require logical reasoning. One such
area is automated theorem proving in formal languages like Lean 4. However, evaluating the correctness of
generated proofs remains a challenge, especially in cases where proofs are partially correct or deviate
structurally from a reference.
Traditional methods, such as compilation success or comparison to a gold-standard proof, offer limited
insight. These approaches fail to capture the model’s internal decision-making and provide little
interpretability. An alternative path is to analyze the model’s attention patterns as a proxy for how it
processes logical structure.
In this work, we propose an interpretable method for assessing the correctness of formal proofs generated
by an LLM, using information from attention heads in a transformer. This not only offers a practical
evaluation signal but may also help identify components within the model that are particularly sensitive to
logical consistency.
Problem statement
We aim to build an interpretable binary classifier that predicts the correctness of formal proofs generated by
large language models, using attention-based topological features extracted from transformer layers.
Challenges
• The method is exploratory, and it is not guaranteed that attention patterns will provide a stable signal for
proof correctness.
• Even if such a signal is found, its generality across different domains of mathematics and formal languages
remains unproven.
• There are no standard datasets with diverse mathematical content and incorrect proofs; existing ones are
limited in scope and often require preprocessing to be usable.
This study is, for now, limited to classifying formal proofs from the domain of propositional logic.
Methods
We use two interpretable features: MTopDiv0, derived from attention-based graphs, and
self-attention values on the end-of-tactic tokens.
Both features are interpretable and reflect the internal connectedness of a proof. MTopDiv0
measures how well the tokens in a proof are coordinated from the perspective of a specific attention
head. End-of-tactic self-attention is inversely related to the tactic’s integration into the overall
structure, offering another proxy for logical consistency.
MTopDiv0 is the normalized total weight of a minimum spanning tree built over an attention-based
graph, where tokens of the same tactic are collapsed into a single node. Lower values correspond to
more coherent proofs. The second feature captures how much the model "attends back" to tactic
ends—lower attention suggests stronger structural integration.
We use the PropL dataset containing synthetic formal proofs. Incorrect proofs are constructed by
recombining tactics from the same problem, based on recorded model trial-and-error attempts.
Attention matrices are used to compute graph-based features after structural masking.
Results
The most discriminative attention head—identified using the MTopDiv0 feature—achieved an AUC of 0.90
when used to classify proofs as correct or incorrect. This shows that attention-based topological structure
can reflect logical correctness in a consistent and interpretable way.
We report the Area Under the ROC Curve (AUC) as the main evaluation metric, since it captures the
tradeoff between sensitivity and specificity without relying on a fixed threshold. A high AUC indicates that
the feature provides a strong correctness signal across different settings.
Results
A heatmap shows the mean absolute difference in MTopDiv0 values between correct and incorrect proofs,
averaged across the dataset. This reveals that several heads in the higher layers demonstrate strong
discriminatory potential.
Research gap
The method has been tested only on propositional logic tasks in Lean 4; its generalizability to other areas
of mathematics, formal languages, and model architectures remains an open question. This matters for
extending the findings beyond Lean 4 and contributing to a broader understanding of reasoning in large
language models.
A possible direction for future research is to use the identified "logical" attention heads for building proof
trees via trial-and-error strategies.
Bibliography
[name]. Formal Proof. Notices of the American Mathematical Society, 2008.
[name], [name]. The Lean 4 Theorem Prover and Programming Language. CADE-28, 2021.
[name], [name]. Generative Language Modeling for Automated Theorem Proving. arXiv preprint, 2020.
[surname], [name], [name], [name], [name], [name], [name]. Manifold Topology Divergence: a
Framework for Comparing Data Manifolds. NeurIPS, 2021.
KomeijiForce. PropL. Hugging Face Datasets, 2025.
Qwen Team. Qwen2.5-Coder-1.5B. Hugging Face, 2025.