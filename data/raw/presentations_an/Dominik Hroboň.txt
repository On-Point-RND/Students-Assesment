Making LLMs Reliable: A
Systematic Study of Uncertainty
[name]
PLAN
1. Introspective Planning: Aligning Robots' Uncertainty
with Inherent Task Ambiguity [location]
2. Decomposing Uncertainty for Large Language Models
through Input Clarification Ensembling [location]
3. DO LLMS ESTIMATE UNCERTAINTY WELL IN
INSTRUCTION-FOLLOWING? [location]
*Clickable links
Introspective PlaNNing: Aligning
Robots' Uncertainty with Inherent
Task Ambiguity
MAIN IDEA
Construct a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected
safe and compliant plans, which are retrieved during deployment. Evaluations on three tasks, demonstrate that
introspection substantially improves both compliance and safety over state-of-the-art LLM-based plaNNing methods.
PROBLEM
The reliability of LLM outputs has direct implications downstream robotics tasks. Language models are prone to
hallucinations, which cause models to generate plans that are at odds with commonsense knowledge, not executable by
the robot, or incompatible with the environment constraints
SOLUTION
• Propose a novel introspective plaNNing
scheme
• Introduce a new, weaкly supervised
offlinе knowledge base construction
method that guides the LLM to
generate human-aligneed introspective
reasoning examples as post-hoc
rationalizations of human-selected
safe-and-compliant plans.
• Create a new Safe Mobile
[location]
UNCERTAINTY ESTIMATION ABILITY IN INSTRUCTION-FOLLOWING?
MAIN IDEA
Researchers proposed a controlled evaluaion methodology using two datasets to enable more accurate
compaison of uncertainty estimation methods
PROBLEM
Existing datasets make it difficult to disentangle sources of uncertainty — for instance, distinguishin
task execution errors from instruction-following errors.
SOLUTION
A new dataset was introduced with two distiinct versions:
Controlled: Confounding factors are minimized, and tasks are categorized by difficulty:
Controlled-Easy: Tasks with clearly distinguishaible correct and incorrect answeers.
Controlled-Hard: Tasks involving more subtle errors.
Realistic: Contains real-world outputs to evaluae model performaNce in more naturaliistic conditions.
CONTRIBUTIONS
• Systematic Evaluation
• Benckmark Dataset
• Findings
UNCERTAINTY ESTIMATION ABILITY IN INSTRUCTION-FOLLOWING ON IFEVAL
DATA BASELINE UNCERTAINTY ESTIMATION METHODS
IFEval dataset
• Verbalized confidence
Each prompt consists of two components: a
task and an instruction, where the instruction
specifies the action
• Normalized p(true) and p(true
• Perplexity and Sequence probability
• Mean token entropy for LLMs
• Probing:
MODELS and METRIC
1. The researchers trained a linear model that predicts the
success of following instructions using internal LLM
states.
2. The results of a program that verifies the execution of an
instruction based on IFEval were used as true labels.
Uncertainty evaluaion results using
IFEval
LLMs struggle to estimate
uncertainty in instruction-following
Sequence probability outpеrforms
perplexity, revealing a length signal
in uncertainty estimation
No model or methoN consistently
excels across instruction types
Problems with datasets with clear
evaluaion criteria
1. Dependence on the length of the
response
2. Confusion between the uncertainty of completing a task and following instructions
3. Different levels of error complexity between models
UNCERTAINTY ESTIMATION ABILITY IN INSTRUCTION-
FOLLOWING ON RESEAChers’S BENCHMARK
CONTROLLED VERSION
Two levels of controlled difficulty–Controlled-
Easy and Controlled-Hard–by generating three
categories of respoNses: completely incorrect,
correct, and subtlly off-targеt.
To neutralize the impact of token length,
reasurchers use GPT-4 to generate both correct
and incorrect respoNses with similar token length
REALISTIC VERSION
Retain the natural length and signals inherent in
rеsроnses generated by multiple LLLMs
(LLaMA2-chat-7B, LLaMA2-chat-13B, Mistral-
7B-Instruct-v0.3, Phi-3-mini-128k, and
LLaMA2-chat-70B).
Token length is not controlled, allowing for the
natural variance found in actual model-generated
responses
COMPARISON OF UNCERTAINTY METHODS: COMPARISON OF UNCERTAINTY METHODS:
CONTROLLED-EASY AND REALISTIC CONTROLLED-HARD
• Probe generally outpеrforme baseline methods in
both Controlled-Easy and Realistic version
• Probe still struggles with uncertainty estimation in
more challenging scenarios
• In Probe, middle layers consisently offer the most
informative representations for uncertainty and self-evaluaion
estimation
• Self-evaluaion methods outpеrform logit-based
ones in Controlled-Easy
• Sequence probability excels, but normalized p(true)
remaiNs a strong contender in Realistic
COMPARISON OF INSTRUCTION TYPES COMPARISON OF MODELS
• The relationship between the success of instructions
and the quality of uncertainty assessment
• The difficult case of "puNctuation" (instructions to
avoid puNctuation)
• LLaMA-2-13B excels in Perplexity
• smaller models, such as Mistral-7B-Instruct and
Phi-3-mini-128k outpеrform the larger LLaMA-2-
chat-13B in self-evaluaion methods
MAIN THOUGHTS
The novelty of the research:
1. The first comprehensive study of uncertainty estimation in LLLM forinstruction execution
tasks.
2. Focus on the tasks of following instructions rather than the actual tasks.
Introduction of a new benckmark:
Two versions: Controlled and Realistic
Analysis results:
1. Self-assessment verbalization methods are effective in simple tasks (Controlled-Easy).
2. The internal states of the model provide reliable signals of uncertainty in Controlled-Easy and Realistic tasks.
3. All methods have diffiiculties with more complex tasks (Controlled-Hard).
Limitations:
1. A narrow range of types of instructions and areas.
2. The risk of data leakage due to similar tasks in the learning process of the model.
Future directions:
Expanding the dataset to cover more areas.Investigation of the causes of LLLM errors in
uncertainty estimation.
THAT’S IT!
THANK YOU 