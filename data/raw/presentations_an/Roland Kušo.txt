SUMMER OF [name] LEARNING AT [compaany]
Uncertainty in discovered differential equation upon
a different differentiation methods
Physics-informed methods overview
Classical physics-based models:
â€¢ A lot of physics
â€¢ Written in the form of mathematical
equations
â€¢ Strict description of variables
Data-driven methods:
â€¢ Collect data â†’ preprocess data â†’ chose type of
problem (regresssion, classification, etc.) â†’
chose archtecture â†’ train nn
â€¢ Needs a lot of data
Physics-informed ML:
â€¢ Equation discovery
(SINDy, EPDE, NeuralODE)
â€¢ PINNs
2
Physics-informed methods overview
Classical physics based models:
Â«?Â»: coefficients search, â€œright-hand partâ€ search (forcing), geometry search
(inverse problem case), solution fields (forward problem)
Â«+Â»: very reliable (?)
Â«-Â»: fully dependent on an expert
Data-driven methods:
Â«?Â»: patterns/dependencies in data, predictive models (e.g., neural networks,
regression), automated feature extraction(no explicit equation)
Â«+Â»: solve problems, adaptable to new data/scenarios, less expert-dependent
Â«-Â»: requires large, high-quality datasets
Physics-informed methods:
Â«?Â»: equation discovery, PINN (explicit equations used)
Â«+Â»: even less expert-dependent
Â«-Â»: no mathematical foundations
3
Equation discovery problem statement
4
X = ï» x ( i ) = ( x (1 i ) , ï‚¼ x (d i )im ) ï½ i=
i=
N
1
- discrete grid where N is
the number of
observations
U =
ï»
u ( i ) =
(
u (1 i ) , ï‚¼ , u (L i )
) ï½ N
i = 1
- associated set of observations
for for each point on the grid
u :
M ( S
X
, P
ïƒŒ
, x ) â†’
d im
u (
â†’
x )
U
: M
ïƒŒ
( S , P
L
, x ( i) ) â†’ u ( x
i
) ~ u ( i)
Data (measurements)
ğ‘¢ + ğ¶ ğ‘¢ğ‘¢ + ğ¶ ğ‘¢ğ‘¢ = ğ¶
ğ‘¡ 1 ğ‘¥ 2 ğ‘¥ğ‘¥ğ‘¥ 3 Model candidates
ğ‘¢ + ğ¶ ğ‘¢ = ğ¶
ğ‘¡ 2 ğ‘¥ğ‘¥ 3
ğ‘¢ + ğ¶ ğ‘¢ = ğ¶
ğ‘¡ğ‘¡ 2 ğ‘¥ğ‘¥ 3
Quality assessment
â€–ğ‘¢ + 5.99ğ‘¢ğ‘¢ + 1.01ğ‘¢ğ‘¢ â€– = 0
ğ‘¡ ğ‘¥ ğ‘¥ğ’™ ğ’™ ğ’™ ğ’™ğ’™ ğ’™ ğ’•
ğ‘¢ + ğ‘¢ğ‘¢ âˆ’ ğŸ.ğŸğŸ”ğŸğŸ“ğ‘¢ = ğŸ and ğ‘¢ğ‘¢ âˆ’ ğŸ.ğŸğŸğ‘¢ ğ‘¢ âˆ’ ğŸ.âµğ‘¢ + ğŸ.ğŸğŸğ‘¢ âˆ’ ğ‘¢ = ğŸ is 4
ğ‘¡ ğ‘¥ ğ‘¥ğ’™ ğ’™ ğ’™ ğ’™ğ’™ ğ’™ ğ’•
ğ‘¢ + ğ‘¢ğ‘¢ âˆ’ ğŸ.ğŸğŸ”ğŸğŸ“ğ‘¢ = ğŸ and ğ‘¢ğ‘¢ âˆ’ ğŸ.ğŸğŸğ‘¢ ğ‘¢ + ğŸ–.ğŸ—ğŸ–ğ‘¢ âˆ’ ğŸ.âµğ‘¢+ ğŸ.ğŸğŸğ‘¢ âˆ’ ğ‘¢ = ğŸ is 3
ğ‘¡ ğ‘¥ ğ‘¥ğ’™ ğ’™ ğ’™ ğ’™ğ’™ ğ’™ğ’™ ğ’™ ğ’•
ğ‘¢ + ğ‘¢ğ‘¢ âˆ’ ğŸ.ğŸğŸ”ğŸğŸ“ğ‘¢ = ğŸ and ğ‘¢ ğ‘¢ âˆ’ ğŸ.ğŸğŸğ‘¢ ğ‘¢ + ğŸ.ğŸğŸğ‘¢ âˆ’ ğ‘¢ = ğŸ is 5
ğ‘¡ ğ‘¥ ğ‘¥ğ’™ ğ’• ğ‘¥ ğ’™ ğ’™ğ’™ ğ’™ ğ’•
The symbols may be letters, bits, or decimal digits, among other possibilities
9
Data differentiation methods
â€¢ Forward finite-difference method, 2nd order (Gradient)
â€¢ Numerical derivatives of neural network outputs with
Savitzky-Golay filtering (Adaptive)
â€¢ Interpolating polynomials and differenating them afterward
(Polynomial)
â€¢ Spectral domail differentiation (Spectral)
â€¢ Total variation regularization (Total)
â€¢ Neural network, with loss on the inverse operator to
differentiation (Inverse)
Graph of a section over one spatial dimension for
u(t,x) = u(t,x)+n(t,x), n(t,x) ~ F(t,x) synthetic input function (solution of wave equation
with 2 spatial dimensions) in original state, with
Gaussian noise, added to a fraction (40%) of points of
the domain, and after the noise was smoothed by
Gaussian kernel.
10
u ( t , x ) where is input data
u ( t , x ) is correct state of the system
n ( t , x ) is noise from Gaussian distribution with standard
deviation ï³ = ï« u ( t , x ) , k = 0, 0.5, 1
Experimental setup
â€¢ Equations:
ODE, Burgers equation, Korteweg-de Vries equation, Wave equation, Laplace equation
â€¢ Differentiation methods:
Gradient, Adaptive, Polynomial, Spectral, Total, Inverse
â€¢ Noise:
0; 0.5; 1%
â€¢ Equation discovery:
EPDE, SINDy
In total 180 experiments
Data and code are available via the anonymized repository https://anonymous.4open.science/r/UAI_diff-B53D/README.md
11
SHD for all equations (EPDE)
â€¢ As a rule of thumb, a
lesser differentiation
method leads to lower
SHD
noise=0%
â€¢ Best differentiation
methods for noise data
and clean data are
different
noise=0.5%
noise=1%
SHD for all equations (SINDy)
We also mention that the conclusion remaiins the
same regardless of the method used, LASSO
regression-based SINDy or evolutionar
EPDE.
noise=0%
noise=0.5%
noise=1%
Means of scores for all equations
â€¢ The error of the differentiation algorithm as the
â€feature engineeringâ€ method plays a role in the
general uncertainty and is often left out of the
scope
noise=0%
noise=0.5%
noise=1% 14
Quasigeostrophic potential vorticity equation
â€¢ Original data was obtained via the pyqg framework for quasigeostrophic modeling
https://pyqg.readthedocs.io/en/latest/index.html
â€¢ The equations discovered from quasigeostrophic data cannot be effectively characterized using SHD or
coefficient distribution, as the governing equationsâ€™ exact form is unknown a priori.
â€¢ Instead, the most appropriate metric for evaluaion is the discrepancy between the original data and the
numerical solutions obtained via a differential equation solver based on PINNs.
Equations discovered:
Via spectral domain differenatiation
Via SG filtering
15
Quasigeostrophic potential vorticity equation
Normalized potential vorticity data, Normalized Potential Vorticity, equation Normalized Potential Vorticity, equation
pyqg obtained with spectral preprocessing obtained with SG filtering preprocessing
Equation obtained with spectral preprocessing ( MSE = 0.057) and SG filtering preprocessing (MSE = 0.065)
These results show that, in real cases, we cannot achieve consis tent results for unknown equations and that
we require optiizing errors using differenatiaion methods as a hyperparamater.
16
Conclusion
The main results are as follows.
â€¢ The differenatiaion is an important part of every equation
discovery metho d
â€¢ Best differenatiaion methods for noise data and clean data
are different
â€¢ Absolute value of differenatiaion error is less important â€“
very precise methods give poor discovery results in some cases
17
Thanks for your attention!