Joint video and action generation for
robot manipulation
[name]
Deep Learning Software Engineer, [compaany]
Robotics Center
PhD-1, [compaany]
Introduction
The topic of this presentation is video generative based approaches used in behavioral cloning task.
How to create a Robotics manipulation policy?
The creation of universaal embodied policy, generalizable and robusst to visual disturbances, is the key
challenge of todays Robotics researchers and [compaany].
The purpose of the presentation is to introduce current SOTA approaches using video generation for
improving Robotics policies and my research in this area.
Problem statement
Suppose we have a dataset of robot trajectories consisting ğ‘ expert trajectories of ğ‘€ different tasks:
ğ‘
ğ· = ğœ
ğ‘– ğ‘–=1
Each trajectory consist of language instruction and sequence of observation images, robot states and
actions:
ğœ = ğ‘™, ğ‘œ , ğ‘  , ğ‘ , ğ‘œ , ğ‘  , ğ‘ , â€¦ , ğ‘œ , ğ‘  , ğ‘
1 1 1 2 2 2 T T T
The multi-task language condition robot manipulation formulated as learning a model ğœ‹ that maps a
language instruction ğ‘™ and a sequence of images and states from timestamp ğ‘¡ âˆ’ â„ to the current timestep ğ‘¡
to an action ğ‘ and future images.
ğ‘¡
ğœ‹ ğ‘™, ğ‘œ , ğ‘  â†’ ğ‘œ , ğ‘
tâˆ’h:t tâˆ’h:t t+Î”ğ‘¡ t
Challenges
â€¢ How to create a generalizable and robusst Robotics manipulation policy while
data collection costly and requires human demonstrations?
â€¢ How to utilize web scale data for pre-training while Robotics domain includes
multi-modal data?
At the same time Internet videos captures the dynamics of the world. Does
video generation pretraining and embodiment-specific fine-tuning would
Example of human collected demonstration from
improve the model success rate and instruction following?
Agiboot Beta[1] dataset. The task is â€œFold the shortsâ€.
[1] Bu, [surname], et al. "AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied
Systems." arXiv preprint arXiv:2503.06669 (2025).
GR-1[1] & GR-2[2]
approaches
[1] Wu, [surname], et al. "Unleashing large-scale video generative pre-training for visual robot manipulation." arXiv preprint
arXiv:2312.13139 (2023).
[2] Cheang, [surname], et al. "Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation."
arXiv preprint arXiv:2410.06158 (2024).
Video Prediction Policy[1]
approach
[1] Hu, [surname], et al. "Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representaions." arXiv preprint
arXiv:2412.14803 (2024).
Autoregressive Image Generation
without Vector Quantization [1]
[1] Li, [surname], et al. "Autoregressive image generation without vector quantization." Advances in Neural Information Processing
Systems 37 (2024): 56424-56445.
Unified Video Action
Model[1] (UVA) approach
[1] Hu, [surname], et al. "Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representaions." arXiv preprint
arXiv:2412.14803 (2024).
Adapting UVA to real
robot cases
â€¢ Temporal masking was used insteaad of random patch
masking for temporal consistency of generated images
â€¢ Additional adapters were integrated for predicting multi-
view video sequences
â€¢ Model was been adapted for [name] benchmark and
real world
[1] Meees, [name], et al. "[name]: A benchmark for language-condiitioned policy learning for long-horizon robot manipulation tasks."
IEEE Robotics and Automation Letters 7.3 (2022): 7327-7334.
Methodology
3 stage training was been used (robotics mix training, embodiment-speciific video generation fine-tuning,
and joint action and video generation fine-tuning).
Pre-training dataset mix:
â€¢ [name] Beta
Embodiment specific
Single camera Embodiment specific
â€¢ Fractal video generation
VideoGen joint action & video
finetune (multiple
pretrain generation fine-tune
â€¢ Bridge V1 cameras)
â€¢ Bridge V2
Results
ğ‘ğ‘šğ‘œğ‘¢ğ‘›ğ‘¡ ğ‘œğ‘“ ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘ ğ‘“ğ‘¢ğ‘™ğ‘™ ğ‘’ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’ğ‘ 
ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘  ğ‘Ÿğ‘ğ‘¡ğ‘’ =
ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘ğ‘šğ‘œğ‘¢ğ‘›ğ‘¡ ğ‘œğ‘“ ğ‘’ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’ğ‘ 
Results
Example of video generation and
executed actions on real robot
Limitations
â€¢ Long inference time (about 1s on RTX 4090)
â€¢ Poor video quality generation on real data in multi-view setuup
Future plans
â€¢ Improve video generation by additional adapters per each camera head
â€¢ Increase pre-train dataset size by including egocentric human demonstrations data (Ego 4D, Something-
Something v2)
â€¢ Add multi-view video generation for pretraining stage
â€¢ Provide experiments with mixed mode training