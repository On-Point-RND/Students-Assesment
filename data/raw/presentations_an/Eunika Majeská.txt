Slide 1
----------------------------------------
[name] [surname]
Junior ML Research Engineer, [compaany]
Junior ML Researcher, [compaany]

Leveraging Knowledge distillation to overcome Model Collapse

Slide 2
----------------------------------------
Introduction

Many modern models are trained on large‑scale internet datasets that already contain significant amounts of synthetic data generated by earlier models.
As synthetic outputs flood web‑crawled corpora, retraining on this mixed data risks progressive performance degradation—an effect known as Model Collapse.
Goal of this talk: to review the challenges of training on self‑generated data and demonstrate how knowledge distillation can mitigate Model Collapse, preserving model fidelity over multiple training generations.



Slide 3
----------------------------------------
Problem statement

Model Collapse: Progressive divergence from true data distribution
Symptoms: Loss of diversity, fine detail, and overall performance
Scope: Vision, NLP, audio—any domain with synthetic‑data loops
Easiest solution: Data accumulation



Slide 4
----------------------------------------
Key idea



Slide 5
----------------------------------------
Data accumulation

Combine newly generated synthetic data with a portion of original real data at each generation step
We also combine it with Knowledge Distillation
The procedure of data accumulation



Slide 6
----------------------------------------
DiT – Experimental Setup
Model: Diffusion Transformer (DiT‑B/4)
Dataset: Subset of ImageNet with 5 classes, 1,300 examples per class (6,500 total).
Steps: 150,000
KD loss function: teacher predictions + cosine distance between vital layers
Evaluation Metrics: FID, LPIPS, and Inception Score (IS) computed on held‑out real images vs. generated samples.



Slide 7
----------------------------------------
DiT – First Step Results



Slide 8
----------------------------------------
DiT – Distribution comparison



Slide 9
----------------------------------------
DiT – Multi‑Step Evaluation
Performance gap widens: over three iterations, FID for synthetic-only balloons, while accumulation+KD grows much more slowly.
Visual collapse: synthetic-only models at step 3 generate monotonous, low‑diversity artifacts.
Our method prevails: combining data accumulation with knowledge distillation preserves both diversity and class fidelity even deep into the collapse pipeline.



Slide 10
----------------------------------------
Summarization – Setup



Slide 11
----------------------------------------
Summarization – Metrics



Slide 12
----------------------------------------
Summarization – Perplexity distribution



Slide 13
----------------------------------------
Summarization – Samples



Slide 14
----------------------------------------
Summarization – Results
Key takeaways:
Pure synthetic training degrades summarization quality over iterations.
Data accumulation and KD each help; their combination delivers the best results.
Distillation preserves both ROUGE performance and healthy token‐level uncertainty.
Qualitative impact:
Fewer repetition errors, more complete coverage of key facts, better fluency.



Slide 15
----------------------------------------
Conclusion

Image Generation (DiT)
	- First‑step FID gap cut by >40 points with KD
	- 3‑step FID stabilized ~30 pts below synthetic‑only
	- Feature‑space drift nearly eliminated (Wasserstein ↓ from 0.026 to 0.013)
Text Summarization (T5)
     	- ROUGE decline slowed from ~2 points to <0.5 points per iteration
     	- Per‑sample perplexity maintained at teacher‑level, heavy outliers removed
Why It Matters:
General: Plug‑and‑play across image and language models
Practical: Guards production pipelines against silent degradation
Future‑Proof: As synthetic data floods the web, this method preserves model’s quality
Next Steps:
How does a language model behave if its pretraining corpus is mostly synthetic, then it’s fine‑tuned on a real task?
What happens when the teacher is smaller or larger than the student? Can a compact teacher still curb collapse, or does a giant teacher give extra resilience?
Audio, video, and real‑time retraining loops where synthetic data is unavoidable.



Slide 16
----------------------------------------
Shane Barratt and Rishi Sharma. A note on the inception score, 2018. URL https://arxiv. org/abs/1801.01973.
Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey Gromov, Daniel A. Roberts, Diyi Yang, David L. Donoho, and Sanmi Koyejo. Is model collapse ineviitable? breaking the curse of recursion by accumulating real and synthetic data, 2024. URL https://arxiv.org/abs/2404.01413.
Ryuichiro Hataya, Han Bao, and Hiromi Arai. Will large-scale generative models corrupt future datasets?, 2023. URL https://arxiv.org/abs/2211.08095.
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv.org/abs/1503.02531.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv. org/abs/2207.12598.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoisinng diffusion probabilistic models, 2020. URL https://arxiv.org/abs/2006.11239.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact language models via pruning and knowledge distillation, 2024. URL https://arxiv.org/abs/2407.14679
William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https://arxiv.org/abs/2212.09748.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023. URL https://arxiv.org/abs/1910.10683.
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget, 2023. URL https://arxiv.org/abs/2305.17493.
Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric, 2018. URL https: //arxiv.org/abs/1801.03924.



Bibliography



