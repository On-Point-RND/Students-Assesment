Prediction of motifs of transcription factor binding sites from amino acid sequences of proteins by artificial intelligence methods
Student [name], [location]
Academic advisor: Ph.D. in Technology
[name]
ICG SB RAS, Lab computer proteomics.
[location] â€“ 2025
Introduction
â€¢ Transcription factors (TF) are regulatory proteins
that control the rate of transcription by binding to
short sections of DNA called transcription factor
binding sites (TFBS).
â€¢ The task of detecting TFBS is one of the most
important task of modern bioinformatics.
â€¢ The detection of TFBS is essential for
understanding transcription regulation in cells.
â€¢ For example, the HOCOMOCCO database
contains 1,120 TF whose motifs are known[1], but
there are about 1,600 in total [2].
[1] https://doi.org/10.1093/nar/gkad1077 - HOCOMOCO in 2024: a
RCSB-PDB: 1A02
rebuild of the curated collection of binding models for human and
mouse transcription factors
[2] 10.1016/j.cell.2018.01.029 - The Human Transcription Factors
2
Introduction
TFBS Search Methods
An experimental approach The computer approach
â€¢ ChIP-seq â€¢ Based on alignment
(CONSENSUS)
â€¢ TF-SELEX â€¢ Based on probabilistic
models (BaMMotif)
â€¢ Electrophoretic mobility â€¢ Based on SVM (Kmer-
shift assay (EMSA) SVM)
â€¢ Based on deep learning
â€¢ DNase footprinting (DeeperBind)
They are based only on the nucleotide
sequence and do not take TF information.
3
Protein language models (pLM)
1. [name]
2. [name]
(a) Secondary Structure (b) Fold Prediction (c) Contact Prediction
Prediction 3. [name]
4. [name] (d) Fluorescence Prediction (e) Solubility Prediction (f) Localization Prediction
[name] A. et al. [name]: Optimized Protein Language Model Unlocks General-Purpose Modelling 2023.
4
Goal and objectives
The goal of the work:
Development of a method for predicting transcription factor binding sites from
protein sequences using generative artificial intelligence models.
Objectives:
1) Preparation of a sample of transcription factors and motifs necessary for
training and evaluating the model.
2) Reprresentation of amino acid sequences in the form of numerical matrices using
the [name] Protein Language Model.
3) The cnoice of a generative AI model for predicting the motifs of binding sites.
4) Training a generative AI model to predict the motives of the binding site.
5) Evaluation of the quality of motifs prediction.
5
Data
PFM
â€¢ The data was taken from databases â€“
[name] v13, [name] (2014), [name].
â€¢ Total unique motifs â€“ 1110 (65% [name],
35% [name]).
â€¢ The motifs of only Vertebrate animals were
selected.
â€¢ Each motif is a positional frequency matrix
(PFM)
6
Approaches to motif generation
g g
n n
â€¦ M i d i d CGGCGCGC
K 0 d d W
L K 5 6 e b e b l e CGTCGCGC L A f i t
Approach â„–1 [name] m m d T o
A A
S
M 2 - e
n
e
n
o M
G
â€¦
GCAAC
S U
L
M
M S i e i a C
E
t m
o
o
r
P D
1 Ã— ğ‘›
ğ‘› Ã— 1280 ğ‘‘ Ã— 1280
g g
n n
â€¦ M i d i d
K L K 0 5 6 d e b d e b l e f i ğ‘› â€“ length of the amino acid sequence;
Approach â„–2 T m m d t o ğ‘‘ â€“ The length of the DNA-binding domain.
A A
S
M 2 - e
n
e
n
o M M
M S i e i a
E
t m
o
o
r
1 Ã— ğ‘› P D
ğ‘› Ã— 1280 ğ‘‘ Ã— 1280
https://github.com/Nikita17012002/Master_diploma/tree/main
7
Model prot2motif
Protein embedding
prev_hidden encoder hiddens
attention
Conv1D
ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ = ğ‘‰ğ‘‡ âˆ— tanh(ğ‘Š ğ‘  + ğ‘ˆ â„ )
scores
ğ‘ ğ‘ ğ‘–âˆ’1 ğ‘ ğ‘—
Linear
exp(ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ )
ğ‘–ğ‘—
ğ›¼ =
attn weights ğ‘–ğ‘— Ïƒğ‘‘
exp(ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ )
ğ‘–ğ‘—
Dropout ğ‘˜=1 ğ‘–ğ‘˜
ğ‘‘
ğ‘ = à· ğ›¼ âˆ— â„
Embbeedded context vector ğ‘– ğ‘–ğ‘— ğ‘—
ğ‘—=1
hidden
GRU
GRU softmax Matrix
encoder_outputs
Encoder Decoder
Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau, 2015 8
The result of training the prot2motif model
Loss function:
ğ‘š ğ‘›
1
ğ‘€ğ‘†ğ¸ = à· à·(ğ´ âˆ’ ğµ )2
ğ‘–ğ‘— ğ‘–ğ‘—
ğ‘š âˆ— ğ‘›
ğ‘–=1 ğ‘—=1
ğ‘š - the length of the motif (ğ‘š=25);
ğ‘› - the number of nucleotides (ğ‘›=5);
ğ´ - the probability of the ğ‘— at position ğ‘–
ğ‘–ğ‘—
source matrix;
ğµ - the probability of the ğ‘— at position ğ‘– the
ğ‘–ğ‘—
generated matrix;
Training parameters:
1) Optimizer â€“ AdamW
2) Learning rate = 0.001
3) Dropout (Encoder) = 0.3
4) Dropout (Decoder) = 0.2
9
The result of training the prot2motif model
The Score-SW metric[3]:
Ïƒğ‘š (2 âˆ’ Ïƒ (ğ´ âˆ’ ğµ )2)
ğ‘–=1 ğ‘—âˆˆ{ğ´,ğ¶,ğº,ğ‘‡} ğ‘–ğ‘— ğ‘–ğ‘—
ğ‘†ğ‘Šğ‘ ğ‘ğ‘œğ‘Ÿe =
2ğ‘š
ğ‘š - the length of the motif (ğ‘š=25);
ğ´ - the probability of the ğ‘— at position ğ‘–
ğ‘–ğ‘—
source matrix;
ğµ - the probability of the ğ‘— at position ğ‘– the
ğ‘–ğ‘—
generated matrix;
The reference model of the motif MA0650.4
The generated motif model MA0650.4
10
[3] https://doi.org/10.1016/j.jmb.2004.02.048 - Constraind Binding Site Diversity within Families of Transcription Factors Enhances Pattern Discovery Bioinformatics
Conclusions
1) A selection of transcription factors and motifs necessaary for training and
evaluaating the model has been prepared.
2) Matrix representations of amino acid sequences were obtained using the
[name] proteiin language model.
3) A generative model of the seq2seq type has been chosen to solve the
prediction problem.
4) The prot2motif generative model has been trained to predict binding site
motifs.
11