Motivation Letter
My research journey in machine learning lies at the intersection of two rapidly evolving domains: code generation with large language models (LLMs) and diffusion models. These two areas, while seemingly distinct, share a deep reliance on structured representations, generative capabilities, and the integration of creativity with mathematical rigor. Over the past few years, I’ve become increasingly fascinated by the mechanisms behind these models — their strengths, their limitations, and the many unsolved problems that continue to drive innovation forward.
I was first introduced to the world of code generation during my bachelor’s studies. This subfield captured my interest because it sits at the confluence of software engineering, natural language processing, and formal logic. Unlike typical NLP tasks that focus on human language, code generation requires an understanding of both syntactic structure and semantic correctness. The process is not just about generating text that “looks right”—it’s about producing output that can compile, execute, and solve a meaningful task. This distinctiveness sparked my curiosity. During my first internship and throughout my bachelor’s thesis, I studied the impact of contextual modifications on the performance of code-generating LLMs. I explored various prompting strategies, experimented with the inclusion or exclusion of surrounding code blocks, and examined how different types of context affect model behavior. Through this, I began to notice recurring patterns: certain modifications consistently led to more accurate or more readable outputs, while others degraded performance in subtle but significant ways. This experience solidified my understanding of the inner workings of transformer-based architectures and how they respond to structured input. It also taught me the importance of fine-grained experimentation and systematic analysis.
As I entered my master’s program at [location], I decided to explore a different yet equally fascinating area: diffusion models. What drew me to this space was the elegance of its mathematical formulation and its rapidly growing impact on creative and generative tasks. Diffusion models, particularly in their recent forms, have demonstrated remarkable performance across image, audio, and video synthesis. However, despite their successes, they also raise many fundamental questions about modeling uncertainty, sampling efficiency, and long-term coherence in generation. My current research centers on gesture generation from audio. In this project, I treat gestures as sequences of human motion data and aim to synthesize them from speech. This task shares many similarities with video generation but introduces unique challenges. The generated motion must not only align with the rhythm and semantics of the audio input, but also preserve the biomechanical plausibility of human movement. In contrast to static image generation, even small artifacts in motion synthesis become highly noticeable due to temporal continuity. One of the most pressing open problems I encountered is the generation of long, coherent sequences — gestures that evolve naturally over time, without abrupt transitions or mechanical repetitions. This challenge has become the central focus of my current research.
What excites me most about working at the intersection of LLMs and diffusion models is how much remains unexplored. Despite the practical orientation of my projects, I find myself continually drawn to the theoretical underpinnings — the “why” behind the “how.” I want to understand what makes certain architectures generalize better, how latent spaces evolve during training, and what mechanisms allow for true creativity in generative models. These are the kinds of questions I hope to explore further.
The primary reason I am applying to attend this program is to determine my long-term research direction. As I approach the end of my master’s degree, I know that I want to pursue a PhD. I’m deeply committed to contributing to machine learning research, but I also recognize that this field is broad and evolving fast. Right now, I’m still in the process of identifying the specific research topic I want to dedicate the next several years of my life to. I believe that exposure to cutting-edge work, through conversations with top researchers and participation in a high-level academic community, will help me find that direction.
This program offers exactly the kind of environment I am looking for: intellectually vibrant, multidisciplinary, and full of people who share a passion for solving hard problems. Having the chance to engage with researchers from top universities and leading industry labs will not only give me insights into the latest methods, but also help me better understand where my skills and interests can have the most impact. These interactions are invaluable — not just for choosing a PhD topic, but for developing a strong academic network and building collaborative relationships that might shape my career for years to come.
Moreover, I see this opportunity as a way to stay grounded in the latest developments. Machine learning is changing rapidly. New models, benchmarks, and ideas appear every month, and staying up-to-date can be overwhelming even for full-time researchers. Participating in programs like this helps me stay in touch with the state of the art, and more importantly, with the community that is building it. I believe that becoming an active part of this community — by attending workshops, giving feedback, presenting ideas, and listening to others — is not just useful, but essential for growth as a researcher.
So, I think that this school will play an important role in determining my future path, and will also help me meet new interesting and important people in the field, expand my horizons and understand new topics. Thank you for considering my application!